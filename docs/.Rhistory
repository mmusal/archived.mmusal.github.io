wd="C:/Users/rm84/Desktop/research/HMM/data/"
setwd(wd)
SummaryAgeSex <- read_excel("SummaryAgeSex.xlsx",
sheet = "SummaryAgeSex")
sagesex=as.data.frame(matrix(nrow=nrow(shape),ncol=nrow(SummaryAgeSex)))
I=nrow(shape)
j=0
for(i in 1:I){
i=2+12*(i-1)
j=1+j
sagesex[j,]=as.numeric(t(SummaryAgeSex[,i]))
}
names(sagesex)=as.character(t(SummaryAgeSex[,1]))
race=read.table(file="DECENNIALPL2020.P2-2023-02-15T024354race.txt",sep="\t",header=TRUE)
#race is going to have 6 main races and 1 remainder
racenames=c(race[1:6,1],"rest")
race1=as.data.frame(matrix(nrow=nrow(race)-2,ncol=ncol(race)-1))
for(j in 2:ncol(race)){race1[1:6,j-1]=race[1:6,j]/race[nrow(race),j]}
for(j in 1:ncol(race1)){
race1[nrow(race1),j]=1-sum(race1[1:(nrow(race1)-1),j])}
race2=t(race1)
colnames(race2)=racenames
#https://censusreporter.org/data/table/?table=B19083&geo_ids=04000US06,050|04000US06&primary_geo_id=04000US06#
#wd="C:/Users/rm84/Desktop/research/HMM/data/"
#setwd(wd)
#GINI=read.table("acs2019_5yr_B19083_05000US06083.csv",header=TRUE,sep=",")
#attach(GINI)
wd="C:/Users/rm84/Desktop/research/HMM/data/"
setwd(wd)
GINI=read.table("gini20and21.csv",header=TRUE,sep=",")
attach(GINI)
library(INLA);
library(spdep);
# validateNb
# check that nbObject is symmetric, has connected components
#
validateNb = function(x) {
if (is.symmetric.nb(x) && n.comp.nb(x)[[1]] < length(x)) return(TRUE);
return(FALSE);
}
# isDisconnected
# check that nbObject has more than 1 component
#
isDisconnected = function(x) {
return(n.comp.nb(x)[[1]] > 1);
}
# indexByComponent
#
# input: vector of component ids
# returns: vector of per-component consecutive node ids
#
indexByComponent = function(x) {
y = x;
comps = as.matrix(table(x));
num_comps = nrow(comps);
for (i in 1:nrow(comps)) {
idx = 1;
rel_idx = 1;
while (idx <= length(x)) {
if (x[idx] == i) {
y[idx] = rel_idx;
rel_idx = rel_idx + 1;
}
idx = idx + 1;
}
}
return(y);
}
# nb2graph
#
# input: nb_object
# returns: dataframe containing num nodes, num edges,
#          and a list of graph edges from node1 to node2.
#
nb2graph = function(x) {
N = length(x);
n_links = 0;
for (i in 1:N) {
if (x[[i]][1] != 0) {
n_links = n_links + length(x[[i]]);
}
}
N_edges = n_links / 2;
node1 = vector(mode="numeric", length=N_edges);
node2 = vector(mode="numeric", length=N_edges);
idx = 0;
for (i in 1:N) {
if (x[[i]][1] > 0) {
for (j in 1:length(x[[i]])) {
n2 = unlist(x[[i]][j]);
if (i < n2) {
idx = idx + 1;
node1[idx] = i;
node2[idx] = n2;
}
}
}
}
return (list("N"=N,"N_edges"=N_edges,"node1"=node1,"node2"=node2));
}
# nb2subgraph
# for a given subcomponent, return graph as lists of node1, node2 pairs
#
# inputs:
# x: nb object
# c_id: subcomponent id
# comp_ids: vector of subcomponent ids
# offsets: vector of subcomponent node numberings
# returns: list of node1, node2 ids
#
nb2subgraph = function(x, c_id, comp_ids, offsets) {
N = length(x);
n_links = 0;
for (i in 1:N) {
if (comp_ids[i] == c_id) {
if (x[[i]][1] != 0) {
n_links = n_links + length(x[i]);
}
}
}
N_edges = n_links / 2;
node1 = vector(mode="numeric", length=N_edges);
node2 = vector(mode="numeric", length=N_edges);
idx = 0;
for (i in 1:N) {
if (comp_ids[i] == c_id) {
if (x[[i]][1] != 0) {
for (j in 1:length(x[[i]])) {
n2 = unlist(x[[i]][j]);
if (i < n2) {
idx = idx + 1;
node1[idx] = offsets[i];
node2[idx] = offsets[n2];
}
}
}
}
}
return (list("node1"=node1,"node2"=node2));
}
# orderByComp
# given nbObject, reorder nb object so that all components are contiguous
# singletons moved to end of list
# returns list containing:
#   - new nbObject
#   - vector which maps old index to new index
#
orderByComponent = function(x) {
if (!validateNb(x)) return(list());
N = length(x);
if (!isDisconnected(x)) return(list(x,seq(1:N)));
rMap = rep(integer(0),N);
comp_ids = n.comp.nb(x)[[2]];
comps = as.matrix(table(comp_ids));
num_comps = nrow(comps);
comp_sizes = as.vector(comps[,1]);
idx = 1;
for (i in 1:nrow(comps)) {
if (comp_sizes[i] > 1) {
positions = which(comp_ids == i);
for (j in 1:length(positions)) {
rMap[idx] = as.integer(positions[j])
idx = idx + 1;
}
}
}
for (i in 1:nrow(comps)) {
if (comp_sizes[i] == 1) {
positions = which(comp_ids == i);
for (j in 1:length(positions)) {
rMap[idx] = as.integer(positions[j])
idx = idx + 1;
}
}
}
new_ids = vector("character", length=N);
for (i in 1:N) {
idx_old = rMap[i];
new_ids[i] = attributes(x)$region.id[idx_old];
}
# generate new nb list
new_nb = structure(vector("list", length=N),class="nb");
attr(new_nb, "region.id") = new_ids;
attr(new_nb, "type") = attributes(x)$type;
attr(new_nb, "sym") = attributes(x)$sym;
attr(new_nb, "region.id") = new_ids;
for (i in 1:N) {
idx_old = rMap[i];
old_nbs = x[[idx_old]];
num_nbs = length(old_nbs);
new_nb[[i]] = vector("integer", length=num_nbs);
for (j in 1:num_nbs) {
old_id = old_nbs[j];
if (old_id == 0) {
new_nb[[i]][j] = as.integer(0);
} else {
new_id = which(rMap == old_id);
new_nb[[i]][j] = as.integer(new_id);
}
}
}
return(list(new_nb,rMap));
}
# reorderVector
#
# input: data vector, offsets vector
# returns: vector of same length as input data vector
#          reordered according to offsets
#
reorderVector = function(x, rMap) {
if (!is.vector(x)) return(NULL);
N = length(x);
result = vector("numeric", length=N);
for (i in 1:N) {
result[i]= x[rMap[i]];
}
return(result);
}
# reorderMatrix
#
# input: data matrix, offsets vector
# returns: matrix of same shape as input data matrix,
#          rows reordered according to offsets
#
reorderMatrix = function(x, rMap) {
if (!is.matrix(x)) return(NULL);
N = nrow(x);
result = matrix(nrow=N, ncol=ncol(x));
for (i in 1:N) {
result[i,]= x[rMap[i],];
}
return(result);
}
# scale_nb_components
#
# input: nb_object
# returns: vector of per-component scaling factor (for BYM2 model)
# scaling factor for singletons is 0
#
scale_nb_components = function(x) {
N = length(x);
comp_ids = n.comp.nb(x)[[2]];
offsets = indexByComponent(comp_ids);
comps = as.matrix(table(comp_ids));
num_comps = nrow(comps);
scales = vector("numeric", length=num_comps);
for (i in 1:num_comps) {
N_subregions = comps[i,1];
scales[i] = 0.0;
if (N_subregions > 1) {
# get adj matrix for this component
drops = comp_ids != i;
nb_tmp = droplinks(x, drops);
nb_graph = nb2subgraph(nb_tmp, i, comp_ids, offsets);
adj.matrix = sparseMatrix( i=nb_graph$node1, j=nb_graph$node2, x=1, dims=c(N_subregions,N_subregions), symmetric=TRUE);
# compute ICAR precision matrix
Q =  Diagonal(N_subregions, rowSums(adj.matrix)) - adj.matrix;
# Add a small jitter to the diagonal for numerical stability (optional but recommended)
Q_pert = Q + Diagonal(N_subregions) * max(diag(Q)) * sqrt(.Machine$double.eps)
# Compute the diagonal elements of the covariance matrix subject to the
# constraint that the entries of the ICAR sum to zero.
Q_inv = inla.qinv(Q_pert, constr=list(A = matrix(1,1,N_subregions),e=0))
# Compute the geometric mean of the variances, which are on the diagonal of Q.inv
scaling_factor = exp(mean(log(diag(Q_inv))))
scales[i] = scaling_factor;
}
}
return(scales);
}
library(INLA)
get_scaling_factor = function(nbs) {
#Build the adjacency matrix
adj.matrix = sparseMatrix(i=nbs$node1,j=nbs$node2,x=1,symmetric=TRUE)
#The ICAR precision matrix (note! This is singular)
Q=  Diagonal(nbs$N, rowSums(adj.matrix)) - adj.matrix
#Add a small jitter to the diagonal for numerical stability (optional but recommended)
Q_pert = Q + Diagonal(nbs$N) * max(diag(Q)) * sqrt(.Machine$double.eps)
# Compute the diagonal elements of the covariance matrix subject to the
# constraint that the entries of the ICAR sum to zero.
#See the function help for further details.
Q_inv = inla.qinv(Q_pert, constr=list(A = matrix(1,1,nbs$N),e=0))
#Compute the geometric mean of the variances, which are on the diagonal of Q.inv
return((mean(log(diag(Q_inv)))))
}
wd='C:/Users/rm84/Desktop/research/HMM/data/'
setwd(wd)
#note that dplyr:: is neccessary for us to communicate to R which package we are
##this does not run before you have other scripts that ran
covid19vaccines=(read.table('covid19vaccinesbycounty.csv',sep=',',header=TRUE))
covid19vaccinessmall=
covid19vaccines%>% dplyr::select(c(county,administered_date,cumulative_fully_vaccinated))
rm(covid19vaccines)
covid19vaccinessmall$WOcc=strftime(covid19vaccinessmall$administered_date,format="%V")
covid19vaccinessmall$YOcc=strftime(covid19vaccinessmall$administered_date,format="%Y")
problemdates202127=c('2021-01-01','2021-01-02','2021-01-03')
problemdates202226=c('2022-01-01','2022-01-02')
problemdates202326=c('2023-01-01')
covid19vaccinessmall$
YOcc[covid19vaccinessmall$administered_date
%in% problemdates202326]<-2022
covid19vaccinessmall$
WOcc[covid19vaccinessmall$administered_date
%in% problemdates202326]<-52
covid19vaccinessmall$
YOcc[covid19vaccinessmall$administered_date
%in% problemdates202127]<-2020
covid19vaccinessmall$
WOcc[covid19vaccinessmall$administered_date
%in% problemdates202127]<-52
covid19vaccinessmall$
YOcc[covid19vaccinessmall$administered_date
%in% problemdates202226]<-2021
covid19vaccinessmall$
WOcc[covid19vaccinessmall$administered_date
%in% problemdates202226]<-52
covid19vaccinessmall$biweekOcc=as.numeric(ceiling((as.numeric((covid19vaccinessmall$WOcc)))/2))
#these problem dates:
covid19vaccinessmall$biweekOcc=ifelse(nchar(covid19vaccinessmall$biweek)==1,
paste0('0',covid19vaccinessmall$biweek),covid19vaccinessmall$biweek)
covid19vaccinessmall=mutate(covid19vaccinessmall,time=as.numeric(paste(YOcc,biweekOcc,sep="")))
##############summarizedcovid needs to be corrected for cumulativeness
problemdates=covid19vaccinessmall[(covid19vaccinessmall$time=='202326'),]
unique(problemdates$administered_date)
###For this time period what needs to happen is
###2022-01-01 and 2022-01-02 which is 202226 needs to be 202126
###2021-01-01 02 and 03 which is 202226 needs to be 202126
NCounties=length(table(covid19vaccinessmall$county))
L=array(dim=c(NCounties))
#for(l in 1:NCounties){
#  L[l]=1+517*(l-1)
#}
covid19vaccinessmall=
covid19vaccinessmall %>% arrange(county, administered_date)
#Find the new number 517 is not correct in 1/23/2023
#1/23/2023 62 values in L
L=unique(covid19vaccinessmall$county)
#the data has the following county values that need to be removed
removed=c('All CA and Non-CA Counties','All CA Counties',
'Outside California','Unknown')
#56792 to 53128
covid19vaccinessmall=covid19vaccinessmall %>%
filter(!county %in% removed)
L=unique(covid19vaccinessmall$county)
numberofdays=nrow(covid19vaccinessmall)/length(L)
NCounties=length(L)
daily_fully=array(dim=c(nrow(covid19vaccinessmall),1))
for(l in 1:(NCounties)){
small= (covid19vaccinessmall[covid19vaccinessmall$county==L[l],])
daily_fully[(1+((l-1)*numberofdays)):((l)*numberofdays),1]=
c(0,diff(small$cumulative_fully_vaccinated))
}
covid19vaccinessmall$daily_fully=daily_fully
problem=covid19vaccinessmall[covid19vaccinessmall$county=='Madera',]
covid19vaccinessmall1=covid19vaccinessmall[,c(-2,-3,-4,-5,-6)]
covid19vaccinessmall2=covid19vaccinessmall1 %>%
group_by(time,county) %>%
summarize(sum_Vac = sum(daily_fully, na.rm = TRUE),.groups="keep")
dataVacWide=covid19vaccinessmall2 %>%
pivot_wider(names_from = time, values_from = sum_Vac)
#row 2 and 3 59 need to be removed
'All CA and Non-CA Counties'
'All CA Counties'
'Outside CA'
'Unknown'
#This does not need to be done, filtered up above. removing
# the non-state
#dataVacWide=dataVacWide[-c(2,3,33,59),]
M=ncol(dataVacWide)-1
P=c(ncol(dataVacWide):2)
for(l in 1:M){
dataVacWide[,P[l]]=rowSums(dataVacWide[,2:P[l]])}
dataVacWide=as.data.frame(dataVacWide)
#we will start the data from the deaths
#assign 0 to the starts at the end
#202003 is the start of the other dataset 1
#202004 202005 202006 202007 202008 202009
#202010 202011 202012 202013 202014 202015
#13 columns of 0 need to be added this is unfortunately manual
##############################################
#remember for this to run you should have already run the newCali r script
N=nrow(GrpdCalifornia1)
#for california fips and county names are ordered equally.
#this can only run after newcaliforniadata.r is run
#this is manual if data changes this also needs to change.
dataVacWide=dataVacWide[,-c(1,2,(ncol(dataVacWide)-1),ncol(dataVacWide))]
start=which(names(dataVacWide)=='202016')
end=which(names(dataVacWide)=='202226')
T=(end-start)+1
numberofbiweeks=T+13
VacPop=array(dim=c(N,T))
for(j in 1:(T)){
VacPop[,j]=dataVacWide[,j]/CASocDemDeaths$population
}
tobeadded<-matrix(nrow=N,ncol=13,data=0)
VacPop=(cbind(tobeadded,VacPop))
#t = 1 to 25 is 2020
#t = 26 to 51 is 2021
#t = 52 to 77 is 2022
CASummary=cbind(CASocDemDeaths,VacPop)
rmarkdown::render("index.Rmd")
setwd('C:/Users/rm84/Documents/GitHub/mmusal.github.io/docs/index.Rmd')
getwd()
setwd("C:/Users/rm84/Documents/GitHub/mmusal.github.io/docs/")
rmarkdown::render("index.Rmd")
head(CASummary)[1:10]
CASummary=cbind(CASocDemDeaths,VacPop)
#be careful shape data is not in order
shapeanddata <- merge(shape, CASummary, by.x = "GEOID", by.y = "ID")
N=nrow(shapeanddata)
#K is number of covariates change it to fit the model, Expected Mortality does not count. Poverty, Income, Need to add density.
######################
#start and end index of deaths
Pop=as.numeric(shapeanddata$population)
TotPop=sum(as.numeric(shapeanddata$population))
#E is expected number of deaths
#######################################################################################33
#same named columns are allowed in R under certain conditions!
#this worked on the merged data
#if facility capacity is no longer part of merge let us relook at the data  t
#this is a little different from combinealldata
#since we do not have to consider ICU capacity
#CASocDemDeaths=CASocDemDeaths[,-c(which(colnames(CASocDemDeaths)=="202027"))]
start=which(names(CASocDemDeaths)=='202003')
end=which(names(CASocDemDeaths)=='202226')
T=(end-start)+1
numberofbiweeks=T
ITER=end-start
#Covariates
#K is number of covariates
K=13+2*T
#creation of covariates matrix
x=array(dim=c(N,K))
#poverty locations I will need to update them for 2022
locationPoverty2020=which(names(CASummary)=='Percent.in.Poverty_2020')
locationPoverty2021=which(names(CASummary)=='Percent.in.Poverty_2021')
#income locations
locationIncome2020=which(names(CASummary)=='Median.Household.Income._2020')
locationIncome2021=which(names(CASummary)=='Median.Household.Income._2021')
###
#insert poverty 2020
x[,1]=(CASummary[,locationPoverty2020])
#insert poverty 2021
x[,2]=(CASummary[,locationPoverty2021])
#this needs to be updated to locationPoverty2022
x[,3]=(CASummary[,locationPoverty2021])
#
#insert income 2020
x[,4]=(CASummary[,locationIncome2020])
#insert income 2021
x[,5]=(CASummary[,locationIncome2021])
#this needs to be updated to locationIncome2022
x[,6]=(CASummary[,locationIncome2021])
#density this only works due to order of shape and CASummary is same
x[,7]=as.numeric(CASummary$population)/shape$ALAND[order(shape$COUNTYFP)]
#PovInc2020
x[,8]=x[,1]*x[,4]
#PovInc2021
x[,9]=x[,2]*x[,5]
#PovInc2022
x[,10]=x[,3]*x[,6]
x[,11]=GINI$Gini_Index_2020
x[,12]=GINI$Gini_Index_2021
x[,13]=GINI$Gini_Index_2021
E=matrix(nrow=N,ncol=numberofbiweeks)
for(j in 1:numberofbiweeks){
E[,j]=(sum(CASocDemDeaths[,start+j-1])/TotPop)*Pop
}
###Target variable
y=array(dim=c(N,T))
for(j in 1:numberofbiweeks){
y[,j]=((CASocDemDeaths[,start+j-1]))
}
for(i in 1:13){
x[,i]=((x[,i])-mean(x[,i]))/sd(x[,i])}
write.csv(CASummary,'C:/Users/rm84/Documents/CASummary.csv')
?read.table
names(CASummary)
View(shape)
View(CASummary)
View(CASummary)
CASummary$'75'
CASummary$'74'
CASummary$'70'
CASummary$'68'
CASummary$'1'
CASummary$'2'
CASummary$'3'
CASummary$'4'
CASummary$'5'
CASummary$'6'
CASummary$'7'
CASummary$'8'
CASummary$'10'
164-154
library(CARBayes)
library(shapefiles)
library(sp)
library(cowplot)
library(viridis)
library(ggpubr)
library("reshape2")
library(ggspatial)
library(ggforce)
library(ggrepel)
library(sf)
library(scales)
E
dim(E)
dim(y)
x
dim(x)
x[1,]
